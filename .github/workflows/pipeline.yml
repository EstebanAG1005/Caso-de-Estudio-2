name: Data Pipeline Automation

on:
  push:
    branches: [ ejercicios ]
  schedule:
    - cron: '0 0 * * *' # Ejecuta el workflow diariamente a medianoche UTC

jobs:
  build-and-save-image:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout Repository
      uses: actions/checkout@v3

    - name: Set up Python 3.10
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'

    - name: Cache pip
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-

    - name: Install Dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt

    - name: Run Data Collection Script
      env:
        API_KEY: ${{ secrets.API_KEY }}
      run: |
        python ./Población_Artificial/data_collection.py

    - name: Run Feature Generation Script
      run: |
        python ./Extracción_de_Features/feature_generation.py

    - name: Run Model Training Script
      run: |
        python ./Generación_Modelo/model_training.py
      # Se asume que el script guarda el modelo en una ubicación como "models/model.pkl"

    - name: Save Model Artifact
      uses: actions/upload-artifact@v3
      with:
        name: model-file
        path: Contenedor/model.pkl # Cambia el path según donde se guarde el modelo en tu repositorio

    - name: Build Docker Image
      run: |
        docker build -t caso_estudio_2 .

    - name: Save Docker Image as TAR
      run: |
        docker save caso_estudio_2 -o caso_estudio_2.tar
      # Guarda la imagen como un archivo .tar

    - name: Upload Docker Image Artifact
      uses: actions/upload-artifact@v3
      with:
        name: docker-image
        path: caso_estudio_2.tar
